{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LMHeadModel, Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "model_name = \"model_name\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "#CSV or text file with pairs of English queries and SQL queries\n",
    "dataset_path = \"english_to_sql_dataset.txt\"\n",
    "\n",
    "# Tokenize and prepare the dataset\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=dataset_path,\n",
    "    block_size=128,  # the block size can be adjusted\n",
    ")\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./english_to_sql_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  #the epochs can be adjusted\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10_000,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./english_to_sql_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c116cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Combine 'instruction' and 'output' columns for tokenization\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m instructions \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstruction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     10\u001b[0m combined_queries \u001b[38;5;241m=\u001b[39m instructions \u001b[38;5;241m+\u001b[39m outputs\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the dataset from the pickle file\n",
    "# file_path = 'NSS_file.pkl'\n",
    "# data = pd.read_pickle(file_path)\n",
    "\n",
    "# # Combine 'instruction' and 'output' columns for tokenization\n",
    "# instructions = data['instruction'].tolist()\n",
    "# outputs = data['output'].tolist()\n",
    "# combined_queries = instructions + outputs\n",
    "\n",
    "# # Tokenize and find unique tokens\n",
    "# unique_tokens = set()\n",
    "# for query in combined_queries:\n",
    "#     # Simple tokenization based on whitespace and basic SQL syntax\n",
    "#     # This will not perfectly tokenize all SQL elements (like strings with spaces, etc.)\n",
    "#     tokens = query.replace('(', ' ( ').replace(')', ' ) ').replace(',', ' , ').split()\n",
    "#     unique_tokens.update(tokens)\n",
    "\n",
    "# # Output the number of unique tokens\n",
    "# len(unique_tokens), unique_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a99ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Check the type of the loaded data to understand its structure\n",
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f162e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few elements of the list to understand its structure\n",
    "#data[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e101f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358931,\n",
       " ['table_44088',\n",
       "  \"'i''m\",\n",
       "  'Approaches',\n",
       "  'Warriors?',\n",
       "  'FirstName',\n",
       "  \"'0643879'\",\n",
       "  '\"Sector',\n",
       "  \"nahuatl'\",\n",
       "  '\"Judy',\n",
       "  \"h√§kkinen'\"])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the data is a list of dictionaries with keys 'instruction' and 'output',\n",
    "# we can extract these and find the unique tokens across all entries.\n",
    "\n",
    "# Initialize an empty set to hold the unique tokens\n",
    "unique_tokens_set = set()\n",
    "\n",
    "# Function to tokenize a SQL query\n",
    "def tokenize_sql(sql):\n",
    "    # A very basic tokenizer splitting on spaces, not accounting for string literals or special characters\n",
    "    return sql.replace('(', ' ( ').replace(')', ' ) ').replace(',', ' , ').split()\n",
    "\n",
    "# Loop through each entry and update the set of unique tokens\n",
    "for entry in data:\n",
    "    if 'instruction' in entry and 'output' in entry:\n",
    "        # Tokenize the 'instruction' and 'output'\n",
    "        instruction_tokens = tokenize_sql(entry['instruction'])\n",
    "        output_tokens = tokenize_sql(entry['output'])\n",
    "        # Update the unique tokens set\n",
    "        unique_tokens_set.update(instruction_tokens)\n",
    "        unique_tokens_set.update(output_tokens)\n",
    "\n",
    "# Now we have a set of unique tokens\n",
    "len(unique_tokens_set), list(unique_tokens_set)[:10]  # Show the number of unique tokens and the first 10 as a sample\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77fc87b3",
   "metadata": {},
   "source": [
    "After loading the dataset using the Hugging Face datasets library, the next steps typically involve preprocessing the data, setting up a model for training, training the model, and then evaluating its performance. Here's a general workflow:\n",
    "\n",
    "Inspect and Understand the Dataset:\n",
    "\n",
    "Before diving into model training, it's crucial to understand the structure and contents of your dataset. Check the features (columns), number of samples, and the format of the data.\n",
    "You can inspect the dataset by viewing the first few samples, for example, print(dataset['train'][0]).\n",
    "Preprocess the Data:\n",
    "\n",
    "Depending on your task (e.g., text classification, text-to-SQL conversion), preprocessing steps may include tokenization, padding, truncation, and converting labels to a suitable format.\n",
    "If your task is sequence-to-sequence (like English to SQL), ensure that both the input text and target text are correctly tokenized.\n",
    "Split the Dataset (if necessary):\n",
    "\n",
    "If your dataset doesn't come with predefined train-validation-test splits, you might need to create these splits yourself.\n",
    "Load and Configure the Model:\n",
    "\n",
    "Choose a model appropriate for your task. Hugging Face offers a wide range of pre-trained models that can be fine-tuned for specific tasks.\n",
    "Load the model using its corresponding model class (e.g., AutoModelForSequenceClassification for classification tasks).\n",
    "Set Up Training Arguments:\n",
    "\n",
    "Use TrainingArguments to configure your training parameters such as batch size, number of epochs, learning rate, etc.\n",
    "Initialize the Trainer:\n",
    "\n",
    "Create an instance of Trainer, providing it with the model, training arguments, training dataset, evaluation dataset, and any data collators or custom configurations.\n",
    "Train the Model:\n",
    "\n",
    "Start the training process by calling trainer.train().\n",
    "Monitor the training progress, and adjust parameters if necessary (based on metrics like loss and accuracy).\n",
    "Save the Trained Model:\n",
    "\n",
    "After training, save your model using model.save_pretrained() for later use or inference.\n",
    "Evaluate the Model:\n",
    "\n",
    "Use the validation or test set to evaluate the model's performance.\n",
    "Analyze the results to understand the model's strengths and weaknesses.\n",
    "Fine-tuning and Hyperparameter Optimization (Optional):\n",
    "\n",
    "Based on the initial performance, you might want to fine-tune your model further or perform hyperparameter optimization.\n",
    "Deployment and Inference (Optional):\n",
    "\n",
    "If your goal is to deploy the model, you can move towards setting up an inference pipeline.\n",
    "Test the model with real-world data to see how it performs in practical scenarios.\n",
    "Throughout these steps, it's important to iteratively inspect the results and make adjustments as needed. Machine learning and NLP tasks often require experimentation with different models, preprocessing techniques, and parameters to achieve the best results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
